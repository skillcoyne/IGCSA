<html xmlns="http://www.w3.org/1999/html" xmlns="http://www.w3.org/1999/html" xmlns="http://www.w3.org/1999/html">
<head>
  <meta name="verify-v1" content="6wo51xu6ARUrklKNbMeXjgoTCvR8I3C7/IESIFp0t10="/>
  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="stylesheet" type="text/css" href="style.css">
  <style type="text/css">
    ol, ul {
      margin: 0.5em;
    }

    li {
      margin: 0;
    }
  </style>
  <title>Frequency-based Insilico Genome Generator</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
</head>
<body>
<div id="wrap">
  <div id="header"><h1>Frequency-based Insilico Genome Generator</h1></div>
  <div id="menu">
    <ul>
      <li><a href="index.shtml">Home</a></li>
    </ul>
  </div>

  <div id="sidebar">
    <h3>FIGG:</h3>
    <ul>
      <li><a href="http://sourceforge.net/projects/insilicogenome/">SF project page</a>
      <li><a href="http://sourceforge.net/projects/insilicogenome/files/">SF download page</a>
      <li><a href="https://github.com/skillcoyne/IGCSA/tree/v1.1">Repository</a>
    </ul>
    <br>

    <h3>Links:</h3>
    <ul>
      <li><a href="http://wwwen.uni.lu/lcsb/research/computational_biology">Computational Biology Group @LCSB</a></li>
      <li><a href="http://wwwen.uni.lu/lcsb">Luxembourg Centre for Systems Biomedicine</a></li>
    </ul>
    <h3>Requirements:</h3>
    <ul>
      <li><a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">Java 1.6+</a></li>
      <li><a href="http://hadoop.apache.org/">Hadoop MapReduce v1.2</a></li>
      <li><a href="https://hbase.apache.org/book/quickstart.html">HBase v0.92</a></li>
    </ul>
  </div>

  <div id="main">
    <!-- BEGIN OF THE BODY -->
    <h2>Introduction</h2>

    <p>
      FIGG is a genome simulation tool that uses known or theorized variation frequency, per a given fragment size and grouped by GC content across a genome to model
      new genomes in FASTA format while tracking applied mutations for use in analysis tools or population simulations.
    </p>

    <p>
      FIGG uses <a href="http://hadoop.apache.org/">Hadoop MapReduce and HBase</a> to scalably and rapidly generate individual genomes.
    </p>
    <br>

    <h2>Instructions for Local Installation</h2>

    <p>
      Hadoop version 1.2 and HBase 0.94 is required to run this on a local cluster or single node setup. Please see the <a href="https://hadoop.apache.org/docs/r1.2.1/single_node_setup.html">documentation</a> provided by Apache in order to
      set up a single node installation of Hadoop.
    </p>

    <p>

    <h3>Setup</h3>
    <ol>
      <li>Download the HBase-Genomes-1.1.jar file from the <a href="http://sourceforge.net/projects/insilicogenome/files/releases">release page</a>.</li>
      <li>Download the GRCh37-loaded.tgz file. This contains <a href="http://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.13/">GRCh37</a> pre-fragmented and a database for "normal" variation frequencies based on 1000Genomes and HapMap.
        Uncompress this file, then load all of the directories into HDFS:
        <pre>hadoop dfs -copyFromLocal /path/to/GRCh37-loaded /my/hdfs/path </pre>
      </li>
      <li> Run a Hadoop job to load all of the database files into HBase:
        <pre>
        hadoop -jar HBase-Genomes-1.1.jar org.lcsb.lu.igcsa.hbase.HBaseUtility -d /my/hdfs/path/to/GRCh37-loaded -c IMPORT
        </pre>
      </li>
      </p>
      <p>
      <h3>Run</h3>
      <li>Each mutate job generates one new genome. A new genome name is required each time.  This job generates mutated fragments and stores them in HBase. This can be run as many times as required.
        <pre>
        hadoop -jar HBase-Genomes-1.1.jar org.lcsb.lu.igcsa.MutateFragments -p GRCh37 -m MyNewGenomeName
        </pre>
      </li>
      <li>
        This step generates all of the FASTA files for each chromosome in any genome identifed by name. This can be run anytime, as many times as needed, after the mutation step.  If you try to generate the same genome twice, any existing files for that genome in HDFS will be overwritten.
        <pre>
        hadoop -jar HBase-Genomes-1.1.jar org.lcsb.lu.igcsa.GenerateFullGenome -g MyNewGenomeName -o /my/hdfs/path
        </pre>
      </li>
    </ol>
    <p>
    The resulting FASTA files can be copied out of HDFS using <pre>hadoop -dfs -copyToLocal ... </pre>
    </p>

    <h4>Optional: Load a New Reference Genome</h4>
    <p>
      There is a job available to generate your own fragmented reference genome tables (e.g. not GRCh37). This will overwrite the tables provided by the tgz file referenced above (genome, chromosome, sequences, small_mutations) so if you have generated any mutated genomes the data will be lost!  All FASTA files must be copied to a directory in HDFS. When running the "Load" step all files in the provided directory will be read, including compressed files.
    </p>
    <pre>
    hadoop -jar HBase-Genomes-1.1.jar org.lcsb.lu.igcsa.LoadFromFASTA SomeName /full/hdfs/path/to/FASTA
    </pre>

    <p>
      Generating your own variation frequencies is also possible.  Please see the R directory under 'fragment-database/normal' to generate files of the appropriate format.  This is not set up as a Hadoop job, so these files can be loaded from the local file system (not HDFS) into HBase by running:
      <pre>
    java -class HBase-Genomes-1.1.jar org.lcsb.lu.igcsa.hbase.ImportVariationData -v /local/path/variation.txt -g /local/path/gc_bin.txt -s /local/path/snv_prob.txt -z /local/path/size_prob.txt -f /local/path/variation_per_bin.txt
      </pre>
    </p>

    <h2>FAQ</h2>
    <dl>
      <!--<dt>How can I cite FIGG?</dt>-->
      <!--<dd>-->
      <!--The small-scale variation fragment mutation has been submitted to <a href="http://bioinformatics.oxfordjournals.org/">Bioinformatics</a>.-->
      <!--<ul>-->
      <!--</ul>-->
      <!--</dd>-->
      <dt>Does FIGG work with all genomes?</dt>
      <dd>
        Currently this has only been tested on human GRCh37. However, the tool itself should work with any genomic data in FASTA format.
        The database tables will need to be generated based on your specific genome and known/hypothesized variation information and the
        scripts available in the source download will require alteration to work for your files or new ones written.
      </dd>
      <dt>What data is provided?</dt>
      <dd>
        The data provided in the database tables were based on analysis of <a href="http://www.1000genomes.org/">1000Genomes</a> and <a href="http://hapmap.ncbi.nlm.nih.gov/">HapMap</a> variations called against human reference <a
          href="http://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/">GRCh37 (hg19)</a>. These variation
        tables can be used with any human genome sequence, however variation frequency may change between releases.
      </dd>
      <dt>Can I include specific variations at known locations?</dt>
      <dd>
        FIGG currently only provides a random variation location based on frequency data. Future implementations will include the ability to specify location based variations.
      </dd>
    </dl>
    <h2>Available Tools</h2>
    <dl>
      <dd>
      <dt>Usage: hadoop -jar HBase-Genomes-1.1.jar [main-class] [args]</dt>
      <ul>
        <li>org.lcsb.lu.igcsa.LoadFromFASTA [genome name] [hdfs path to FASTA file directory]</li>
        <li>org.lcsb.lu.igcsa.hbase.HBaseUtility -d [hdfs directory for read/write] -c [IMPORT|EXPORT] -t [comma separated list of tables OPTIONAL]</li>
        <li>org.lcsb.lu.igcsa.MutateFragments -p [reference genome, ex. GRCh37] -m [new genome name]</li>
        <li>org.lcsb.lu.igcsa.GenerateFullGenome -g [reference genome name, ex. GRCh37] -o [hdfs output path for FASTA files]
        </li>
      </ul>
      </dd>
      <dt>Run on Amazon Elastic MapReduce:</dt>
      <dl>
        <p>
        A shell script, <i>run_mutate_aws.sh</i> which uses the Amazon Command Line tool (CLI) and <a href="https://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-cli-install.html">Ruby elastic-mapreduce (EMR)</a> to submit and run the jobs.  For this it is assumed that you will upload all of the tables provided to a folder in S3.  The job specified will load all of the tables into HBase, run the Mutate jobs, export the database tables that were changed (genome,sequence,chromosome) and Generate FASTA files (in S3).  Please see Amazon Web Services documentation for information on setting up your AWS account, security, and S3.
        </p>
        <p>
          This script sets up Spot Instances for less expensive runs, however please keep in mind that running these jobs is charged to the owner of the account, this is not provided by us.
        </p>
      </dl>
    <h2>Software License</h2>
    <dt><a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License 2.0</a></dt>
    <pre>
      Copyright 2013 University of Luxembourg and the Luxembourg Centre for Systems Biomedicine

      Licensed under the Apache License, Version 2.0 (the "License");
      you may not use this file except in compliance with the License.
      You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License.
      </pre>
    </p>

    <!-- END OF THE BODY-->
  </div>
  <!-- BEGIN OF THE FOOTER -->
  <div style="clear: both;"></div>
  <div id="footer">
    <hr/>
    <p style="text-align: center;">
      This work is supported by an AFR grant from the <a href="http://www.fnr.lu/">Fonds National de la Recherche Luxembourg</a>
      <br>
      Copyright 2013 <a href="http://uni.lu">University of Luxembourg</a> and the <a href="http://wwwen.uni.lu/lcsb">Luxembourg Centre for Systems Biomedicine</a>
      <br>
      Last modified: 2013-05-13</p>
  </div>
  <!-- END OF THE FOOTER -->
</div>
</body>
</html>
