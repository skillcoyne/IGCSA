<html xmlns="http://www.w3.org/1999/html" xmlns="http://www.w3.org/1999/html" xmlns="http://www.w3.org/1999/html">
<head>
  <meta name="verify-v1" content="6wo51xu6ARUrklKNbMeXjgoTCvR8I3C7/IESIFp0t10="/>
  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="stylesheet" type="text/css" href="style.css">
  <style type="text/css">
    ol, ul {
      margin: 0.5em;
    }

    li {
      margin: 0;
    }
  </style>
  <title>Frequency-based Insilico Genome Generator</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
</head>
<body>
<div id="wrap">
  <div id="header"><h1>Frequency-based Insilico Genome Generator</h1></div>
  <div id="menu">
    <ul>
      <li><a href="index.shtml">Home</a></li>
    </ul>
  </div>

  <div id="sidebar">
    <h3>FIGG:</h3>
    <ul>
      <li><a href="http://sourceforge.net/projects/insilicogenome/">SF project page</a>
      <li><a href="http://sourceforge.net/projects/insilicogenome/files/">SF download page</a>
      <li><a href="https://github.com/skillcoyne/IGCSA/">Source Repository</a>
    </ul>
    <br>

    <h3>Links:</h3>
    <ul>
      <li><a href="http://wwwen.uni.lu/lcsb/research/computational_biology">Computational Biology Group @LCSB</a></li>
      <li><a href="http://wwwen.uni.lu/lcsb">Luxembourg Centre for Systems Biomedicine</a></li>
    </ul>
    <h3>Requirements:</h3>
    <ul>
      <li><a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">Java 1.6+</a></li>
      <li><a href="http://hadoop.apache.org/">Apache Hadoop MapReduce v1.2</a></li>
      <li><a href="https://hbase.apache.org/book/quickstart.html">HBase v0.92</a></li>
    </ul>
  </div>

  <div id="main">
    <!-- BEGIN OF THE BODY -->
    <h2>Introduction</h2>

    <p>
      FIGG is a genome simulation tool that uses known or theorized variation frequency, per a given fragment size and grouped by GC content across a genome to model
      new genomes in FASTA format while tracking applied mutations for use in analysis tools or population simulations.
    </p>

    <p>
      FIGG uses <a href="http://hadoop.apache.org/">Apache MapReduce and HBase</a> to rapidly generate individual genomes and allow users to scale up generation to fit specific project needs.
    </p>
    <br>

    <h2>Instructions for Local Installation</h2>

    <p>
      Hadoop version 1.2 and HBase 0.94 is required to run this on a local cluster or single node setup. Please see the <a href="https://hadoop.apache.org/docs/r1.2.1/single_node_setup.html">documentation</a> provided by Apache in order to
      set up a single node installation of Hadoop.
    </p>

    <p>

    <h3>Setup</h3>
    <ol>
      <li>Download the <a href="http://sourceforge.net/projects/insilicogenome/files/releases/HBase-Genomes-1.1.jar/download">HBase-Genomes-1.1.jar</a> file from the releases page.</li>
      <li>Download <a href="http://sourceforge.net/projects/insilicogenome/files/Databases/normal-freq-hbase.tgz/download">normal-freq-hbase.tgz</a>. This contains the "normal" variation frequency database based on 1000Genomes and HapMap.<br>
        Uncompress and load the directories (not single files!) into HDFS (or s3) and load into HBase:
        <pre>hadoop dfs -copyFromLocal /path/to/normal-freq /my/hdfs/path </pre>
        <pre>hadoop jar HBase-Genomes-1.1.jar hbaseutil -d /my/hdfs/path/to/normal-freq -c IMPORT</pre>

      <li>Download FASTA files for GRCh37 (or other human reference release).  <a href="http://hgdownload.cse.ucsc.edu/">UCSC</a> provides these as separate chromosomes, ignore the chr*_random and  chrUn_* files.  Load these into HDFS (or s3):
        <pre>hadoop dfs -copyFromLocal /path/to/FASTAfiles /my/hdfs/path </pre>
      Then run the 'fastaload' job:
      <pre>hadoop jar HBase-Genomes-1.1.jar fastaload -g [genome name] -f [hdfs/s3 path to FASTA file directory]</pre>
      If you are running on AWS make sure you also run the hbaseutil EXPORT job immediately following this.
      </li>

      <h3>Run</h3>
      <li>Each mutate job generates one new genome. A new genome name is required each time. This job generates mutated fragments and stores them in HBase. This can be run as many times as required.
        <pre>
        hadoop jar HBase-Genomes-1.1.jar mutate -p GRCh37 -m MyNewGenomeName
        </pre>
      </li>
      <li>
        This step generates all of the FASTA files for each chromosome in any genome identifed by name. This can be run anytime, as many times as needed, after the mutation step. If you try to generate the same genome twice, any existing
        files for that genome in HDFS will be overwritten.
        <pre>
        hadoop jar HBase-Genomes-1.1.jar gennormal -g MyNewGenomeName -o /my/hdfs/path
        </pre>
      </li>
    </ol>
    <p>
      The resulting FASTA files can be copied out of HDFS using
    <pre>hadoop -dfs -copyToLocal ... </pre>
    </p>

    <h4>Optional: Generate your own variation frequencies</h4>
    <p>
      Please see the R directory under 'fragment-database/normal' to generate files of the appropriate format. This is not set up as a Hadoop job, so these files can be loaded from the local file system (not HDFS) into HBase by running:
      <pre>
    java -class HBase-Genomes-1.1.jar org.lcsb.lu.igcsa.hbase.ImportVariationData \
    -v /local/path/variation.txt \
    -g /local/path/gc_bin.txt \
    -s /local/path/snv_prob.txt \
    -z /local/path/size_prob.txt \
    -f /local/path/variation_per_bin.txt
      </pre>
    </p>

    <h2>FAQ</h2>
    <dl>
      <!--<dt>How can I cite FIGG?</dt>-->
      <!--<dd>-->
      <!--The small-scale variation fragment mutation has been submitted to <a href="http://bioinformatics.oxfordjournals.org/">Bioinformatics</a>.-->
      <!--<ul>-->
      <!--</ul>-->
      <!--</dd>-->
      <dt>Does FIGG work with all genomes?</dt>
      <dd>
        Currently this has only been tested on human GRCh37. However, the tool itself will work with any genomic data in FASTA format.
        The database tables will need to be generated based on your specific genome and known/hypothesized variation information and the
        scripts available in the source download will require alteration to work for your files or new ones written.
      </dd>
      <dt>What data is provided?</dt>
      <dd>
        The data provided in the database tables were based on analysis of <a href="http://www.1000genomes.org/">1000Genomes</a> and <a href="http://hapmap.ncbi.nlm.nih.gov/">HapMap</a> variations called against human reference <a
          href="http://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/">GRCh37 (hg19)</a>. These variation
        tables can be used with any human genome sequence, however variation frequency may change between releases.
      </dd>
      <dt>Can I include specific variations at known locations?</dt>
      <dd>
        FIGG currently only provides a random variation location based on frequency data. Future implementations will include the ability to specify location based variations.
      </dd>
    </dl>
    <h2>Available Tools</h2>
    <dl>
      <dd>
      <dt>Usage: hadoop jar HBase-Genomes-1.1.jar [program name] [args]</dt>
      <ul>
        <li>fastaload<br>
          -g [genome name] -f [hdfs path to FASTA file directory]
        </li>
        <li>hbaseutil<br>
          -d [hdfs directory for read/write] -c [IMPORT|EXPORT] -t [comma separated list of tables OPTIONAL]
        </li>
        <li>mutate<br>
          -p [reference genome, ex. GRCh37] -m [new genome name]
        </li>
        <li>gennormal<br>
          -g [genome name, ex. GRCh37] -o [hdfs output path for FASTA files]
        </li>
      </ul>
      </dd>
    </dl>

    <dt>Run on Amazon Elastic MapReduce:</dt>
    <dl>
      <p>
        Several shell scripts are provided with the source files which use the Amazon Command Line tool (CLI) and <a href="https://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-cli-install.html">Ruby elastic-mapreduce (EMR)</a> to submit
        and run the jobs. For this it is assumed that you will upload all of the tables provided to a folder in S3. The job specified will load all of the tables into HBase, run the specified jobs, and export the database tables (genome,sequence,chromosome,small_mutations) or generate new FASTA files (in S3).
      </p>
      <p>
        These scripts set up Spot Instances for less expensive runs, however please keep in mind that running these jobs is charged to the owner of the account, this is not provided by us.
      </p>
      <p>
        Please see Amazon Web Services documentation for information on setting up your AWS account, security, and S3.
      </p>

    </dl>
    <h2>Software License</h2>
    <dt><a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License 2.0</a></dt>
    <pre>
      Copyright 2013 University of Luxembourg and the Luxembourg Centre for Systems Biomedicine

      Licensed under the Apache License, Version 2.0 (the "License");
      you may not use this file except in compliance with the License.
      You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License.
      </pre>
    </p>

    <!-- END OF THE BODY-->
  </div>
  <!-- BEGIN OF THE FOOTER -->
  <div style="clear: both;"></div>
  <div id="footer">
    <hr/>
    <p style="text-align: center;">
      This work is supported by an AFR grant from the <a href="http://www.fnr.lu/">Fonds National de la Recherche Luxembourg</a>
      <br>
      Copyright 2013 <a href="http://uni.lu">University of Luxembourg</a> and the <a href="http://wwwen.uni.lu/lcsb">Luxembourg Centre for Systems Biomedicine</a>
      <br>
      Last modified: 2013-05-13</p>
  </div>
  <!-- END OF THE FOOTER -->
</div>
</body>
</html>
